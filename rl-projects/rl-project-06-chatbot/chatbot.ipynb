{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pickle\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from __future__ import print_function # for backward compatibility\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "import random\n",
    "\n",
    "#needed for tensorflow to access gpu\n",
    "import os\n",
    "os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/bin\")\n",
    "import tensorflow as tf\n",
    "#pip install tensorflow-addons==0.11.2\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parser\n",
    "This parser cleans and preprocesses the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses all words into script\n",
    "def parse_all_words(all_words_path):\n",
    "    raw_movie_lines = open('data/movie_lines.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')[:-1]\n",
    "\n",
    "    with codecs.open(all_words_path, \"w\", encoding='utf-8', errors='ignore') as f:\n",
    "        for line in raw_movie_lines:\n",
    "            line = line.split(' +++$+++ ')\n",
    "            utterance = line[-1]\n",
    "            f.write(utterance + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function filter words by preprocessing word counts and creating vocab based on word count theresholds.\n",
    "def preProBuildWordVocab(word_count_threshold=5, all_words_path='data/all_words.txt'):\n",
    "    # borrowed this function from NeuralTalk\n",
    "\n",
    "    if not os.path.exists(all_words_path):\n",
    "        parse_all_words(all_words_path)\n",
    "\n",
    "    corpus = open(all_words_path, 'r').read().split('\\n')[:-1]\n",
    "    captions = np.asarray(corpus, dtype=object) #np.object_ depreciated  changed to object\n",
    "\n",
    "    captions = map(lambda x: x.replace('.', ''), captions)\n",
    "    captions = map(lambda x: x.replace(',', ''), captions)\n",
    "    captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "    captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "    captions = map(lambda x: x.replace('?', ''), captions)\n",
    "    captions = map(lambda x: x.replace('!', ''), captions)\n",
    "    captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "    captions = map(lambda x: x.replace('/', ''), captions)\n",
    "\t\n",
    "\t\n",
    "    print('preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold))\n",
    "    word_counts = {}\n",
    "    # iterate through the captions and create vocab\n",
    "    nsents = 0\n",
    "    for sent in captions:\n",
    "        nsents += 1\n",
    "        for w in sent.lower().split(' '):\n",
    "            \n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print('filtered words from %d to %d' % (len(word_counts), len(vocab)))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '<pad>'\n",
    "    ixtoword[1] = '<bos>'\n",
    "    ixtoword[2] = '<eos>'\n",
    "    ixtoword[3] = '<unk>'\n",
    "\n",
    "    wordtoix = {}\n",
    "    wordtoix['<pad>'] = 0\n",
    "    wordtoix['<bos>'] = 1\n",
    "    wordtoix['<eos>'] = 2\n",
    "    wordtoix['<unk>'] = 3\n",
    "\n",
    "    for idx, w in enumerate(vocab):\n",
    "        wordtoix[w] = idx+4\n",
    "        ixtoword[idx+4] = w\n",
    "\n",
    "    word_counts['<pad>'] = nsents\n",
    "    word_counts['<bos>'] = nsents\n",
    "    word_counts['<eos>'] = nsents\n",
    "    word_counts['<unk>'] = nsents\n",
    "\n",
    "    bias_init_vector = np.array([1.0 * word_counts[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "\n",
    "    return wordtoix, ixtoword, bias_init_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts only the defined vocab from the data\n",
    "def refine(data):\n",
    "    words = re.findall(\"[a-zA-Z'-]+\", data)\n",
    "    words = [\"\".join(word.split(\"'\")) for word in words]\n",
    "    # words = [\"\".join(word.split(\"-\")) for word in words]\n",
    "    data = ' '.join(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and store utterance dictionary\n",
    "def data_parser():\n",
    "    parse_all_words('data/all_words.txt')\n",
    "\n",
    "    raw_movie_lines = open('data/movie_lines.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')[:-1]\n",
    "    \n",
    "    utterance_dict = {}\n",
    "    with codecs.open('data/tokenized_all_words.txt', \"w\", encoding='utf-8', errors='ignore') as f:\n",
    "        for line in raw_movie_lines:\n",
    "            line = line.split(' +++$+++ ')\n",
    "            line_ID = line[0]\n",
    "            utterance = line[-1]\n",
    "            utterance_dict[line_ID] = utterance\n",
    "            utterance = \" \".join([refine(w) for w in utterance.lower().split()])\n",
    "            f.write(utterance + '\\n')\n",
    "    pickle.dump(utterance_dict, open('data/utterance_dict', 'wb'), True)\n",
    "\n",
    "#run data parser\n",
    "    data_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor\n",
    "Extracts vocabulary from movie conversation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 13.607280254364014 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time() # get current time\n",
    "\n",
    "#get files\n",
    "raw_movie_conversations = open('data/movie_conversations.txt', 'r').read().split('\\n')[:-1]\n",
    "utterance_dict = pickle.load(open('data/utterance_dict', 'rb'))\n",
    "corpus = word2vec.Text8Corpus(\"data/tokenized_all_words.txt\")\n",
    "\n",
    "#create and save word vector\n",
    "word_vector_size = 300\n",
    "word_vector = word2vec.Word2Vec(corpus, vector_size=word_vector_size)\n",
    "word_vector.wv.save_word2vec_format(u\"model/word_vector.bin\", binary=True)\n",
    "\n",
    "#load work vector\n",
    "word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "\n",
    "#output how long it took\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extract only the vocabulary part of the data \"\"\"\n",
    "def refine(data):\n",
    "    words = re.findall(\"[a-zA-Z'-]+\", data)\n",
    "    words = [\"\".join(word.split(\"'\")) for word in words]\n",
    "    # words = [\"\".join(word.split(\"-\")) for word in words]\n",
    "    data = ' '.join(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len conversation 83097\n",
      "con_count 1000, traindata_count 2049\n",
      "con_count 2000, traindata_count 3996\n",
      "con_count 3000, traindata_count 6425\n",
      "con_count 4000, traindata_count 8353\n",
      "con_count 5000, traindata_count 10654\n",
      "con_count 6000, traindata_count 12707\n",
      "con_count 7000, traindata_count 14666\n",
      "con_count 8000, traindata_count 16673\n",
      "con_count 9000, traindata_count 18578\n",
      "con_count 10000, traindata_count 20317\n",
      "con_count 11000, traindata_count 22826\n",
      "con_count 12000, traindata_count 25611\n",
      "con_count 13000, traindata_count 27879\n",
      "con_count 14000, traindata_count 30057\n",
      "con_count 15000, traindata_count 32631\n",
      "con_count 16000, traindata_count 34686\n",
      "con_count 17000, traindata_count 36849\n",
      "con_count 18000, traindata_count 38890\n",
      "con_count 19000, traindata_count 41103\n",
      "con_count 20000, traindata_count 43175\n",
      "con_count 21000, traindata_count 45123\n",
      "con_count 22000, traindata_count 47305\n",
      "con_count 23000, traindata_count 48998\n",
      "con_count 24000, traindata_count 51571\n",
      "con_count 25000, traindata_count 53672\n",
      "con_count 26000, traindata_count 55744\n",
      "con_count 27000, traindata_count 57609\n",
      "con_count 28000, traindata_count 59333\n",
      "con_count 29000, traindata_count 61598\n",
      "con_count 30000, traindata_count 63880\n",
      "con_count 31000, traindata_count 65773\n",
      "con_count 32000, traindata_count 67893\n",
      "con_count 33000, traindata_count 70072\n",
      "con_count 34000, traindata_count 72632\n",
      "con_count 35000, traindata_count 75087\n",
      "con_count 36000, traindata_count 77276\n",
      "con_count 37000, traindata_count 79020\n",
      "con_count 38000, traindata_count 81055\n",
      "con_count 39000, traindata_count 83212\n",
      "con_count 40000, traindata_count 85462\n",
      "con_count 41000, traindata_count 87717\n",
      "con_count 42000, traindata_count 90933\n",
      "con_count 43000, traindata_count 93730\n",
      "con_count 44000, traindata_count 95726\n",
      "con_count 45000, traindata_count 97907\n",
      "con_count 46000, traindata_count 100056\n",
      "con_count 47000, traindata_count 102594\n",
      "con_count 48000, traindata_count 104500\n",
      "con_count 49000, traindata_count 106753\n",
      "con_count 50000, traindata_count 108974\n",
      "con_count 51000, traindata_count 111087\n",
      "con_count 52000, traindata_count 113508\n",
      "con_count 53000, traindata_count 115709\n",
      "con_count 54000, traindata_count 117896\n",
      "con_count 55000, traindata_count 119670\n",
      "con_count 56000, traindata_count 121593\n",
      "con_count 57000, traindata_count 123375\n",
      "con_count 58000, traindata_count 125332\n",
      "con_count 59000, traindata_count 127916\n",
      "con_count 60000, traindata_count 130063\n",
      "con_count 61000, traindata_count 132035\n",
      "con_count 62000, traindata_count 134442\n",
      "con_count 63000, traindata_count 136628\n",
      "con_count 64000, traindata_count 138860\n",
      "con_count 65000, traindata_count 140717\n",
      "con_count 66000, traindata_count 143000\n",
      "con_count 67000, traindata_count 145005\n",
      "con_count 68000, traindata_count 147070\n",
      "con_count 69000, traindata_count 149109\n",
      "con_count 70000, traindata_count 151472\n",
      "con_count 71000, traindata_count 153787\n",
      "con_count 72000, traindata_count 155705\n",
      "con_count 73000, traindata_count 157753\n",
      "con_count 74000, traindata_count 160029\n",
      "con_count 75000, traindata_count 162029\n",
      "con_count 76000, traindata_count 164162\n",
      "con_count 77000, traindata_count 166159\n",
      "con_count 78000, traindata_count 168425\n",
      "con_count 79000, traindata_count 170328\n",
      "con_count 80000, traindata_count 172567\n",
      "con_count 81000, traindata_count 174927\n",
      "con_count 82000, traindata_count 176857\n",
      "con_count 83000, traindata_count 178575\n",
      "Time Elapsed: 4.614930152893066 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "conversations = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1 #test length of conversation and raise error if failed\n",
    "\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a = utterance_dict[conversation[i+1]].strip()\n",
    "        con_b = utterance_dict[conversation[i]].strip()\n",
    "        if len(con_a.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b))\n",
    "            traindata_count += 1\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/reversed_conversations_lenmax22', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_a_ind 10, max_b_ind 0\n",
      "max_a 22, max_b 22, avg_a 7.424128170416531, avg_b 7.383465165974871\n"
     ]
    }
   ],
   "source": [
    "#statistics of data\n",
    "max_a = -1\n",
    "max_b = -1\n",
    "max_a_ind = -1\n",
    "max_b_ind = -1\n",
    "sum_a = 0.\n",
    "sum_b = 0.\n",
    "\n",
    "len_a_list = []\n",
    "len_b_list = []\n",
    "\n",
    "for i in range(len(conversations)):\n",
    "    len_a = len(conversations[i][0])\n",
    "    len_b = len(conversations[i][1].split())\n",
    "    if len_a > max_a:\n",
    "        max_a = len_a\n",
    "        max_a_ind = i\n",
    "    if len_b > max_b:\n",
    "        max_b = len_b\n",
    "        max_b_ind = i\n",
    "    sum_a += len_a\n",
    "    sum_b += len_b\n",
    "\n",
    "    len_a_list.append(len_a)\n",
    "    len_b_list.append(len_b)\n",
    "\n",
    "np.save(\"data/reversed_lenmax22_a_list\", np.array(len_a_list))\n",
    "np.save(\"data/reversed_lenmax22_b_list\", np.array(len_b_list))\n",
    "\n",
    "print(\"max_a_ind {}, max_b_ind {}\".format(max_a_ind, max_b_ind))\n",
    "print(\"max_a {}, max_b {}, avg_a {}, avg_b {}\".format(max_a, max_b, sum_a/len(conversations), sum_b/len(conversations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len conversation 83097\n",
      "con_count 1000, traindata_count 1942\n",
      "con_count 2000, traindata_count 3782\n",
      "con_count 3000, traindata_count 6095\n",
      "con_count 4000, traindata_count 7888\n",
      "con_count 5000, traindata_count 10079\n",
      "con_count 6000, traindata_count 12042\n",
      "con_count 7000, traindata_count 13890\n",
      "con_count 8000, traindata_count 15781\n",
      "con_count 9000, traindata_count 17571\n",
      "con_count 10000, traindata_count 19183\n",
      "con_count 11000, traindata_count 21543\n",
      "con_count 12000, traindata_count 24160\n",
      "con_count 13000, traindata_count 26267\n",
      "con_count 14000, traindata_count 28381\n",
      "con_count 15000, traindata_count 30743\n",
      "con_count 16000, traindata_count 32700\n",
      "con_count 17000, traindata_count 34766\n",
      "con_count 18000, traindata_count 36692\n",
      "con_count 19000, traindata_count 38768\n",
      "con_count 20000, traindata_count 40717\n",
      "con_count 21000, traindata_count 42566\n",
      "con_count 22000, traindata_count 44584\n",
      "con_count 23000, traindata_count 46159\n",
      "con_count 24000, traindata_count 48564\n",
      "con_count 25000, traindata_count 50524\n",
      "con_count 26000, traindata_count 52474\n",
      "con_count 27000, traindata_count 54240\n",
      "con_count 28000, traindata_count 55877\n",
      "con_count 29000, traindata_count 57972\n",
      "con_count 30000, traindata_count 60124\n",
      "con_count 31000, traindata_count 61895\n",
      "con_count 32000, traindata_count 63927\n",
      "con_count 33000, traindata_count 66007\n",
      "con_count 34000, traindata_count 68427\n",
      "con_count 35000, traindata_count 70730\n",
      "con_count 36000, traindata_count 72786\n",
      "con_count 37000, traindata_count 74451\n",
      "con_count 38000, traindata_count 76352\n",
      "con_count 39000, traindata_count 78404\n",
      "con_count 40000, traindata_count 80502\n",
      "con_count 41000, traindata_count 82607\n",
      "con_count 42000, traindata_count 85710\n",
      "con_count 43000, traindata_count 88318\n",
      "con_count 44000, traindata_count 90168\n",
      "con_count 45000, traindata_count 92245\n",
      "con_count 46000, traindata_count 94259\n",
      "con_count 47000, traindata_count 96661\n",
      "con_count 48000, traindata_count 98455\n",
      "con_count 49000, traindata_count 100616\n",
      "con_count 50000, traindata_count 102737\n",
      "con_count 51000, traindata_count 104773\n",
      "con_count 52000, traindata_count 107028\n",
      "con_count 53000, traindata_count 109106\n",
      "con_count 54000, traindata_count 111156\n",
      "con_count 55000, traindata_count 112837\n",
      "con_count 56000, traindata_count 114695\n",
      "con_count 57000, traindata_count 116409\n",
      "con_count 58000, traindata_count 118216\n",
      "con_count 59000, traindata_count 120675\n",
      "con_count 60000, traindata_count 122696\n",
      "con_count 61000, traindata_count 124541\n",
      "con_count 62000, traindata_count 126814\n",
      "con_count 63000, traindata_count 128903\n",
      "con_count 64000, traindata_count 131005\n",
      "con_count 65000, traindata_count 132740\n",
      "con_count 66000, traindata_count 134845\n",
      "con_count 67000, traindata_count 136684\n",
      "con_count 68000, traindata_count 138620\n",
      "con_count 69000, traindata_count 140538\n",
      "con_count 70000, traindata_count 142818\n",
      "con_count 71000, traindata_count 144937\n",
      "con_count 72000, traindata_count 146752\n",
      "con_count 73000, traindata_count 148717\n",
      "con_count 74000, traindata_count 150846\n",
      "con_count 75000, traindata_count 152730\n",
      "con_count 76000, traindata_count 154747\n",
      "con_count 77000, traindata_count 156613\n",
      "con_count 78000, traindata_count 158773\n",
      "con_count 79000, traindata_count 160557\n",
      "con_count 80000, traindata_count 162630\n",
      "con_count 81000, traindata_count 164841\n",
      "con_count 82000, traindata_count 166668\n",
      "con_count 83000, traindata_count 168293\n",
      "Time Elapsed: 6.269817113876343 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "conversations = []\n",
    "# former_sents = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1\n",
    "    con_a_1 = ''\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a_2 = utterance_dict[conversation[i]]\n",
    "        con_b = utterance_dict[conversation[i+1]]\n",
    "        if len(con_a_1.split()) <= 22 and len(con_a_2.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = \"{} {}\".format(con_a_1, con_a_2)\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b, con_a_2))\n",
    "            # former_sents.append(con_a_2)\n",
    "            traindata_count += 1\n",
    "        con_a_1 = con_a_2\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/conversations_lenmax22_formersents2_with_former', 'wb'), True)\n",
    "# pickle.dump(former_sents, open('data/conversations_lenmax22_former_sents', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len conversation 83097\n",
      "con_count 1000, traindata_count 1942\n",
      "con_count 2000, traindata_count 3782\n",
      "con_count 3000, traindata_count 6095\n",
      "con_count 4000, traindata_count 7888\n",
      "con_count 5000, traindata_count 10079\n",
      "con_count 6000, traindata_count 12042\n",
      "con_count 7000, traindata_count 13890\n",
      "con_count 8000, traindata_count 15781\n",
      "con_count 9000, traindata_count 17571\n",
      "con_count 10000, traindata_count 19183\n",
      "con_count 11000, traindata_count 21543\n",
      "con_count 12000, traindata_count 24160\n",
      "con_count 13000, traindata_count 26267\n",
      "con_count 14000, traindata_count 28381\n",
      "con_count 15000, traindata_count 30743\n",
      "con_count 16000, traindata_count 32700\n",
      "con_count 17000, traindata_count 34766\n",
      "con_count 18000, traindata_count 36692\n",
      "con_count 19000, traindata_count 38768\n",
      "con_count 20000, traindata_count 40717\n",
      "con_count 21000, traindata_count 42566\n",
      "con_count 22000, traindata_count 44584\n",
      "con_count 23000, traindata_count 46159\n",
      "con_count 24000, traindata_count 48564\n",
      "con_count 25000, traindata_count 50524\n",
      "con_count 26000, traindata_count 52474\n",
      "con_count 27000, traindata_count 54240\n",
      "con_count 28000, traindata_count 55877\n",
      "con_count 29000, traindata_count 57972\n",
      "con_count 30000, traindata_count 60124\n",
      "con_count 31000, traindata_count 61895\n",
      "con_count 32000, traindata_count 63927\n",
      "con_count 33000, traindata_count 66007\n",
      "con_count 34000, traindata_count 68427\n",
      "con_count 35000, traindata_count 70730\n",
      "con_count 36000, traindata_count 72786\n",
      "con_count 37000, traindata_count 74451\n",
      "con_count 38000, traindata_count 76352\n",
      "con_count 39000, traindata_count 78404\n",
      "con_count 40000, traindata_count 80502\n",
      "con_count 41000, traindata_count 82607\n",
      "con_count 42000, traindata_count 85710\n",
      "con_count 43000, traindata_count 88318\n",
      "con_count 44000, traindata_count 90168\n",
      "con_count 45000, traindata_count 92245\n",
      "con_count 46000, traindata_count 94259\n",
      "con_count 47000, traindata_count 96661\n",
      "con_count 48000, traindata_count 98455\n",
      "con_count 49000, traindata_count 100616\n",
      "con_count 50000, traindata_count 102737\n",
      "con_count 51000, traindata_count 104773\n",
      "con_count 52000, traindata_count 107028\n",
      "con_count 53000, traindata_count 109106\n",
      "con_count 54000, traindata_count 111156\n",
      "con_count 55000, traindata_count 112837\n",
      "con_count 56000, traindata_count 114695\n",
      "con_count 57000, traindata_count 116409\n",
      "con_count 58000, traindata_count 118216\n",
      "con_count 59000, traindata_count 120675\n",
      "con_count 60000, traindata_count 122696\n",
      "con_count 61000, traindata_count 124541\n",
      "con_count 62000, traindata_count 126814\n",
      "con_count 63000, traindata_count 128903\n",
      "con_count 64000, traindata_count 131005\n",
      "con_count 65000, traindata_count 132740\n",
      "con_count 66000, traindata_count 134845\n",
      "con_count 67000, traindata_count 136684\n",
      "con_count 68000, traindata_count 138620\n",
      "con_count 69000, traindata_count 140538\n",
      "con_count 70000, traindata_count 142818\n",
      "con_count 71000, traindata_count 144937\n",
      "con_count 72000, traindata_count 146752\n",
      "con_count 73000, traindata_count 148717\n",
      "con_count 74000, traindata_count 150846\n",
      "con_count 75000, traindata_count 152730\n",
      "con_count 76000, traindata_count 154747\n",
      "con_count 77000, traindata_count 156613\n",
      "con_count 78000, traindata_count 158773\n",
      "con_count 79000, traindata_count 160557\n",
      "con_count 80000, traindata_count 162630\n",
      "con_count 81000, traindata_count 164841\n",
      "con_count 82000, traindata_count 166668\n",
      "con_count 83000, traindata_count 168293\n",
      "Time Elapsed: 6.469294548034668 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "conversations = []\n",
    "# former_sents = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1\n",
    "    con_a_1 = ''\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a_2 = utterance_dict[conversation[i]]\n",
    "        con_b = utterance_dict[conversation[i+1]]\n",
    "        if len(con_a_1.split()) <= 22 and len(con_a_2.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = \"{} {}\".format(con_a_1, con_a_2)\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b))\n",
    "            # former_sents.append(con_a_2)\n",
    "            traindata_count += 1\n",
    "        con_a_1 = con_a_2\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/conversations_lenmax22_former_sents2', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len conversation 83097\n",
      "con_count 1000, traindata_count 2049\n",
      "con_count 2000, traindata_count 3996\n",
      "con_count 3000, traindata_count 6425\n",
      "con_count 4000, traindata_count 8353\n",
      "con_count 5000, traindata_count 10654\n",
      "con_count 6000, traindata_count 12707\n",
      "con_count 7000, traindata_count 14666\n",
      "con_count 8000, traindata_count 16673\n",
      "con_count 9000, traindata_count 18578\n",
      "con_count 10000, traindata_count 20317\n",
      "con_count 11000, traindata_count 22826\n",
      "con_count 12000, traindata_count 25611\n",
      "con_count 13000, traindata_count 27879\n",
      "con_count 14000, traindata_count 30057\n",
      "con_count 15000, traindata_count 32631\n",
      "con_count 16000, traindata_count 34686\n",
      "con_count 17000, traindata_count 36849\n",
      "con_count 18000, traindata_count 38890\n",
      "con_count 19000, traindata_count 41103\n",
      "con_count 20000, traindata_count 43175\n",
      "con_count 21000, traindata_count 45123\n",
      "con_count 22000, traindata_count 47305\n",
      "con_count 23000, traindata_count 48998\n",
      "con_count 24000, traindata_count 51571\n",
      "con_count 25000, traindata_count 53672\n",
      "con_count 26000, traindata_count 55744\n",
      "con_count 27000, traindata_count 57609\n",
      "con_count 28000, traindata_count 59333\n",
      "con_count 29000, traindata_count 61598\n",
      "con_count 30000, traindata_count 63880\n",
      "con_count 31000, traindata_count 65773\n",
      "con_count 32000, traindata_count 67893\n",
      "con_count 33000, traindata_count 70072\n",
      "con_count 34000, traindata_count 72632\n",
      "con_count 35000, traindata_count 75087\n",
      "con_count 36000, traindata_count 77276\n",
      "con_count 37000, traindata_count 79020\n",
      "con_count 38000, traindata_count 81055\n",
      "con_count 39000, traindata_count 83212\n",
      "con_count 40000, traindata_count 85462\n",
      "con_count 41000, traindata_count 87717\n",
      "con_count 42000, traindata_count 90933\n",
      "con_count 43000, traindata_count 93730\n",
      "con_count 44000, traindata_count 95726\n",
      "con_count 45000, traindata_count 97907\n",
      "con_count 46000, traindata_count 100056\n",
      "con_count 47000, traindata_count 102594\n",
      "con_count 48000, traindata_count 104500\n",
      "con_count 49000, traindata_count 106753\n",
      "con_count 50000, traindata_count 108974\n",
      "con_count 51000, traindata_count 111087\n",
      "con_count 52000, traindata_count 113508\n",
      "con_count 53000, traindata_count 115709\n",
      "con_count 54000, traindata_count 117896\n",
      "con_count 55000, traindata_count 119670\n",
      "con_count 56000, traindata_count 121593\n",
      "con_count 57000, traindata_count 123375\n",
      "con_count 58000, traindata_count 125332\n",
      "con_count 59000, traindata_count 127916\n",
      "con_count 60000, traindata_count 130063\n",
      "con_count 61000, traindata_count 132035\n",
      "con_count 62000, traindata_count 134442\n",
      "con_count 63000, traindata_count 136628\n",
      "con_count 64000, traindata_count 138860\n",
      "con_count 65000, traindata_count 140717\n",
      "con_count 66000, traindata_count 143000\n",
      "con_count 67000, traindata_count 145005\n",
      "con_count 68000, traindata_count 147070\n",
      "con_count 69000, traindata_count 149109\n",
      "con_count 70000, traindata_count 151472\n",
      "con_count 71000, traindata_count 153787\n",
      "con_count 72000, traindata_count 155705\n",
      "con_count 73000, traindata_count 157753\n",
      "con_count 74000, traindata_count 160029\n",
      "con_count 75000, traindata_count 162029\n",
      "con_count 76000, traindata_count 164162\n",
      "con_count 77000, traindata_count 166159\n",
      "con_count 78000, traindata_count 168425\n",
      "con_count 79000, traindata_count 170328\n",
      "con_count 80000, traindata_count 172567\n",
      "con_count 81000, traindata_count 174927\n",
      "con_count 82000, traindata_count 176857\n",
      "con_count 83000, traindata_count 178575\n",
      "Time Elapsed: 4.362311363220215 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "conversations = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a = utterance_dict[conversation[i]]\n",
    "        con_b = utterance_dict[conversation[i+1]]\n",
    "        if len(con_a.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b))\n",
    "            traindata_count += 1\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/conversations_lenmax22', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reader\n",
    "This reader generates trainable batches from the preprocessed training text from the data parser script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Reader:\n",
    "    def __init__(self, cur_train_index=0, load_list=False):\n",
    "        self.training_data = pickle.load(open('data/conversations_lenmax22_formersents2_with_former', 'rb'))\n",
    "        self.data_size = len(self.training_data)\n",
    "        if load_list:\n",
    "            self.shuffle_list = pickle.load(open('data/shuffle_index_list', 'rb'))\n",
    "        else:    \n",
    "            self.shuffle_list = self.shuffle_index()\n",
    "        self.train_index = cur_train_index\n",
    "\n",
    "    # get batch number from data\n",
    "    def get_batch_num(self, batch_size):\n",
    "        return self.data_size // batch_size\n",
    "\n",
    "    #shuffle index from data\n",
    "    def shuffle_index(self):\n",
    "        shuffle_index_list = random.sample(range(self.data_size), self.data_size)\n",
    "        pickle.dump(shuffle_index_list, open('data/shuffle_index_list', 'wb'), True)\n",
    "        return shuffle_index_list\n",
    "\n",
    "    #generate batch indeices based on batch numbers\n",
    "    def generate_batch_index(self, batch_size):\n",
    "        if self.train_index + batch_size > self.data_size:\n",
    "            batch_index = self.shuffle_list[self.train_index:self.data_size]\n",
    "            self.shuffle_list = self.shuffle_index()\n",
    "            remain_size = batch_size - (self.data_size - self.train_index)\n",
    "            batch_index += self.shuffle_list[:remain_size]\n",
    "            self.train_index = remain_size\n",
    "        else:\n",
    "            batch_index = self.shuffle_list[self.train_index:self.train_index+batch_size]\n",
    "            self.train_index += batch_size\n",
    "        return batch_index\n",
    "\n",
    "    # generate training batches\n",
    "    def generate_training_batch(self, batch_size):\n",
    "        batch_index = self.generate_batch_index(batch_size)\n",
    "        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n",
    "        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n",
    "        return batch_X, batch_Y\n",
    "\n",
    "    # generate training batches with previous former batches\n",
    "    def generate_training_batch_with_former(self, batch_size):\n",
    "        batch_index = self.generate_batch_index(batch_size)\n",
    "        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n",
    "        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n",
    "        former = [self.training_data[i][2] for i in batch_index]    # batch_size of former utterance\n",
    "        return batch_X, batch_Y, former\n",
    "\n",
    "    # generate testing batches\n",
    "    def generate_testing_batch(self, batch_size):\n",
    "        batch_index = self.generate_batch_index(batch_size)\n",
    "        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n",
    "        return batch_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper\n",
    "seq2seq dialog generator is used for the reverse model of backward entropy loss. This determines the reward for semantic coherence in the policy gradient dialogue. In other words it helps represent future reward based on LSTM (i.e encoding, decoding and generating builds). Feature extraction script gets features and characteristics from data to help improve training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RL is set to true a scaler is computed based on semantic coherence and ease of answering loss caption.\n",
    "@tf.function\n",
    "def model_inputs(embed_dim, reinforcement= False):#tensorflow v1 issues.\n",
    "    word_vectors = tf.placeholder(tf.float32, [None, None, embed_dim], name = \"word_vectors\")\n",
    "    reward = tf.placeholder(tf.float32, shape = (), name = \"rewards\")\n",
    "    caption = tf.placeholder(tf.int32, [None, None], name = \"captions\")\n",
    "    caption_mask = tf.placeholder(tf.float32, [None, None], name = \"caption_masks\")\n",
    "    if reinforcement: # Normal training returns only the word_vectors, caption and caption_mask placeholders, \n",
    "        # With reinforcement learning, there is an extra placeholder for rewards\n",
    "        return word_vectors, caption, caption_mask, reward\n",
    "    else:\n",
    "        return word_vectors, caption, caption_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs encoding for sequence to sequence network. The input sequence is passed into the encoder and returns both an ouput RNN and the state\n",
    "def encoding_layer(word_vectors, lstm_size, num_layers, keep_prob, vocab_size):\n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(lstm_size), keep_prob) for _ in range(num_layers)])\n",
    "    outputs, state = tf.nn.rnn_cell.dynamic_rnn(cells, word_vectors, dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process for decoder using LSTM cells and encoder states \n",
    "def decode_train(enc_state, dec_cell, dec_input, \n",
    "                         target_sequence_length,output_sequence_length,\n",
    "                         output_layer, keep_prob):\n",
    "    #Apply dropout to the LSTM cell\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    #Training helper for decoder\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_input, target_sequence_length)\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, enc_state, output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=output_sequence_length)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates an inference decoder that makes use of greedy helpter which feeds last output of decoder as next decoder input.\n",
    "# returns training logits and sample id\n",
    "def decode_generate(encoder_state, dec_cell, dec_embeddings,\n",
    "                         target_sequence_length,output_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    #dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    dec_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    #Decoder helper for inference\n",
    "    #helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, tf.fill([batch_size], 1), 2)\n",
    "    helper = tfa.seq2seq.GreedyEmbeddingSampler(dec_embeddings, tf.fill([batch_size], 1), 2)\n",
    "    \n",
    "    #decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, encoder_state, output_layer)\n",
    "    decoder = tfa.seq2seq.BasicDecoder(dec_cell, helper, encoder_state, output_layer)\n",
    "    \n",
    "    #outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=output_sequence_length)\n",
    "    outputs, _, _ = tfa.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=output_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decodes the encoded layer\n",
    "def decoding_layer(dec_input, enc_state,\n",
    "                   target_sequence_length,output_sequence_length,\n",
    "                   lstm_size,\n",
    "                   num_layers,n_words,\n",
    "                   batch_size, keep_prob,embedding_size, Train = True):\n",
    "    target_vocab_size = n_words\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size,embedding_size], -0.1, 0.1), name='Wemb')\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(lstm_size) for _ in range(num_layers)])\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    if Train:\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            train_output = decode_train(enc_state, \n",
    "                                                cells, \n",
    "                                                dec_embed_input, \n",
    "                                                target_sequence_length, output_sequence_length,\n",
    "                                                output_layer, \n",
    "                                                keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=tf.AUTO_REUSE):\n",
    "        infer_output = decode_generate(enc_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, target_sequence_length,\n",
    "                                           output_sequence_length,\n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "    if Train:\n",
    "        return train_output, infer_output\n",
    "    return infer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends the index corresponding to <bos> which is the beginning of a sentence to the first index of the capton tensor for every batch.\n",
    "def bos_inclusion(caption,batch_size):\n",
    " \n",
    "    sliced_target = tf.strided_slice(caption, [0,0], [batch_size, -1], [1,1])\n",
    "    concat = tf.concat([tf.fill([batch_size, 1],1), sliced_target],1)\n",
    "    return concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an array of size maxlen from every question by padding with zeros or trucating where applicable\n",
    "def pad_sequences(questions, sequence_length =22):\n",
    "    lengths = [len(x) for x in questions]\n",
    "    num_samples = len(questions)\n",
    "    x = np.zeros((num_samples, sequence_length)).astype(int)\n",
    "    for idx, sequence in enumerate(questions):\n",
    "        if not len(sequence):\n",
    "            continue  # empty list/array was found\n",
    "        truncated  = sequence[-sequence_length:]\n",
    "\n",
    "        truncated = np.asarray(truncated, dtype=int)\n",
    "\n",
    "        x[idx, :len(truncated)] = truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only takes non numerical data\n",
    "def refine(data):\n",
    "    words = re.findall(\"[a-zA-Z'-]+\", data)\n",
    "    words = [\"\".join(word.split(\"'\")) for word in words]\n",
    "    data = ' '.join(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batches to feed into network from word vector representation\n",
    "def make_batch_input(batch_input, input_sequence_length, embed_dims, word2vec):\n",
    "    \n",
    "    for i in range(len(batch_input)):\n",
    "        \n",
    "        batch_input[i] = [word2vec[w] if w in word2vec else np.zeros(embed_dims) for w in batch_input[i]]\n",
    "        if len(batch_input[i]) >input_sequence_length:\n",
    "            batch_input[i] = batch_input[i][:input_sequence_length]\n",
    "        else:\n",
    "            for _ in range(input_sequence_length - len(batch_input[i])):\n",
    "                batch_input[i].append(np.zeros(embed_dims))\n",
    "    return np.array(batch_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(target,symbols):  #Remove symbols from sequence\n",
    "    for symbol in symbols:\n",
    "        target = list(map(lambda x: x.replace(symbol,''),target))\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_target(batch_target, word_to_index, target_sequence_length):\n",
    "    target = batch_target\n",
    "    target = list(map(lambda x: '<bos> ' + x, target))\n",
    "    symbols = ['.', ',', '\"', '\\n','?','!','\\\\','/']\n",
    "    target = replace(target, symbols)\n",
    "\n",
    "    for idx, each_cap in enumerate(target):\n",
    "        word = each_cap.lower().split(' ')\n",
    "        if len(word) < target_sequence_length:\n",
    "            target[idx] = target[idx] + ' <eos>'  #Append the end of symbol symbol \n",
    "        else:\n",
    "            new_word = ''\n",
    "            for i in range(target_sequence_length-1):\n",
    "                new_word = new_word + word[i] + ' '\n",
    "            target[idx] = new_word + '<eos>'\n",
    "            \n",
    "    target_index = [[word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in \n",
    "                          sequence.lower().split(' ')] for sequence in target]\n",
    "    #print(target_index[0])\n",
    "    \n",
    "    caption_matrix = pad_sequences(target_index,target_sequence_length)\n",
    "    caption_matrix = np.hstack([caption_matrix, np.zeros([len(caption_matrix), 1])]).astype(int)\n",
    "    caption_masks = np.zeros((caption_matrix.shape[0], caption_matrix.shape[1]))\n",
    "    nonzeros = np.array(list(map(lambda x: (x != 0).sum(), caption_matrix)))\n",
    "    #print(nonzeros)\n",
    "    #print(caption_matrix[1])\n",
    "    \n",
    "    for ind, row in enumerate(caption_masks): #Set the masks as an array of ones where actual words exist and zeros otherwise\n",
    "        row[:nonzeros[ind]] = 1                 \n",
    "        #print(row)\n",
    "    print(caption_masks[0])\n",
    "    print(caption_matrix[0])\n",
    "    return caption_matrix,caption_masks   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_batch(generic_responses, batch_size, word_to_index, target_sequence_length):\n",
    "    size = len(generic_responses) \n",
    "    if size > batch_size:\n",
    "        generic_responses = generic_responses[:batch_size]\n",
    "    else:\n",
    "        for j in range(batch_size - size):\n",
    "            generic_responses.append('')\n",
    "    return make_batch_target(generic_responses, word_to_index, target_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentences from the predicted indices of word with the next highest probability\n",
    "def index2sentence(generated_word_index, prob_logit, ixtoword):\n",
    "    generated_word_index = list(generated_word_index)\n",
    "    for i in range(len(generated_word_index)):\n",
    "        if generated_word_index[i] == 3 or generated_word_index[i] == 0:\n",
    "            sort_prob_logit = sorted(prob_logit[i])\n",
    "            curindex = np.where(prob_logit[i] == sort_prob_logit[-2])[0][0]\n",
    "            count = 1\n",
    "            while curindex <= 3:\n",
    "                curindex = np.where(prob_logit[i] == sort_prob_logit[(-2)-count])[0][0]\n",
    "                count += 1\n",
    "\n",
    "            generated_word_index[i] = curindex\n",
    "\n",
    "    generated_words = []\n",
    "    for ind in generated_word_index:\n",
    "        generated_words.append(ixtoword[ind])    \n",
    "    generated_sentence = ' '.join(generated_words)\n",
    "    generated_sentence = generated_sentence.replace('<bos> ', '')  #Replace the beginning of sentence tag\n",
    "    generated_sentence = generated_sentence.replace('<eos>', '')   #Replace the end of sentence tag\n",
    "    generated_sentence = generated_sentence.replace('--', '')      #Replace the other symbols predicted\n",
    "    generated_sentence = generated_sentence.split('  ')\n",
    "    for i in range(len(generated_sentence)):       #Begin sentences with Upper case \n",
    "        generated_sentence[i] = generated_sentence[i].strip()\n",
    "        if len(generated_sentence[i]) > 1:\n",
    "            generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + '.'\n",
    "        else:\n",
    "            generated_sentence[i] = generated_sentence[i].upper()\n",
    "    generated_sentence = ' '.join(generated_sentence)\n",
    "    generated_sentence = generated_sentence.replace(' i ', ' I ')\n",
    "    generated_sentence = generated_sentence.replace(\"i'm\", \"I'm\")\n",
    "    generated_sentence = generated_sentence.replace(\"i'd\", \"I'd\")\n",
    "\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Chatbot\n",
    "This is the policy gradient model which combines reinforcement learning with cross-entropy loss. Policy gradient is based on LSTM encoder-decoder. policy gradient is stochastic based on a probability distribution of actions depending on the state. The policy gradient loss to minimise is also defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our chatbot class for our model\n",
    "class Chatbot():\n",
    "    def __init__(self, embed_dim, vocab_size, lstm_size, batch_size, input_sequence_length, target_sequence_length, learning_rate =0.0001, keep_prob = 0.5, num_layers = 1, policy_gradients = False, Training = True):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_sequence_length = tf.fill([self.batch_size],input_sequence_length+1)\n",
    "        self.target_sequence_length = tf.fill([self.batch_size],target_sequence_length+1)\n",
    "        self.output_sequence_length = target_sequence_length +1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.policy_gradients = policy_gradients\n",
    "        self.Training = Training\n",
    "\n",
    "    # if policy gradient requested we input accordingly\n",
    "    def build_model(self):\n",
    "        if self.policy_gradients:\n",
    "            word_vectors, caption, caption_mask, rewards = model_inputs(self.embed_dim, True)\n",
    "            place_holders = {'word_vectors': word_vectors,\n",
    "                'caption': caption,\n",
    "                'caption_mask': caption_mask, \"rewards\": rewards\n",
    "                             }\n",
    "        else:\n",
    "            word_vectors, caption, caption_mask = model_inputs(self.embed_dim)\n",
    "            \n",
    "            place_holders = {'word_vectors': word_vectors,\n",
    "                'caption': caption,\n",
    "                'caption_mask': caption_mask}\n",
    "        enc_output, enc_state = encoding_layer(word_vectors, self.lstm_size, self.num_layers,\n",
    "                                         self.keep_prob, self.vocab_size)\n",
    "        #dec_inp = bos_inclusion(caption, self.batch_size)\n",
    "        dec_inp = caption\n",
    "\n",
    "        # gets our inference layer\n",
    "        if not self.Training:\n",
    "            print(\"Test mode\")\n",
    "            inference_out = decoding_layer(dec_inp, enc_state,self.target_sequence_length, \n",
    "                                                    self.output_sequence_length,\n",
    "                                                    self.lstm_size, self.num_layers,\n",
    "                                                    self.vocab_size, self.batch_size,\n",
    "                                                  self.keep_prob, self.embed_dim, False)\n",
    "            logits = tf.identity(inference_out.rnn_output, name = \"train_logits\")\n",
    "            predictions = tf.identity(inference_out.sample_id, name = \"predictions\")\n",
    "            return place_holders, predictions, logits \n",
    "        #gets our loss layer\n",
    "        train_out, inference_out = decoding_layer(dec_inp, enc_state,self.target_sequence_length, \n",
    "                                                    self.output_sequence_length,\n",
    "                                                    self.lstm_size, self.num_layers,\n",
    "                                                    self.vocab_size, self.batch_size,\n",
    "                                                  self.keep_prob, self.embed_dim)\n",
    "\n",
    "        training_logits = tf.identity(train_out.rnn_output, name = \"train_logits\")\n",
    "        prediction_logits = tf.identity(inference_out.sample_id, name = \"predictions\")\n",
    "        \n",
    "        cross_entropy = tf.contrib.seq2seq.sequence_loss(training_logits, caption, caption_mask)\n",
    "        losses = {\"entropy\": cross_entropy}\n",
    "\n",
    "        # will either minimise cross entropy loss or policy gradient loss depending on our state\n",
    "        if self.policy_gradients:\n",
    "            pg_loss = tf.contrib.seq2seq.sequence_loss(training_logits, caption, caption_mask*rewards)\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(pg_loss)\n",
    "            losses.update({\"pg\":pg_loss}) \n",
    "        else:\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(cross_entropy)\n",
    "                \n",
    "        return optimizer, place_holders,prediction_logits,training_logits, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "This is the policy gradient model which combines reinforcement learning with cross-entropy loss. Policy gradient is based on LSTM encoder-decoder. policy gradient is stochastic based on a probability distribution of actions depending on the state. The policy gradient loss to minimise is also defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load either the foward or revese sequence to sequence model\n",
    "def train(type_, epochs=50, checkpoint=False):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    if type_ == \"forward\":\n",
    "        path = \"model/forward/seq2seq\"\n",
    "        dr = Data_Reader(load_list=False) #forward\n",
    "    else:\n",
    "        dr = Data_Reader(load_list=True) #reverse\n",
    "        path = \"model/reverse/seq2seq\"\n",
    "    # create vocab\n",
    "    word_to_index, index_to_word, _ = preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
    "    #load wor to vector model from gensim\n",
    "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "\n",
    "    #build chatbot model and pass in variables defined earlier.\n",
    "    model = Chatbot(dim_wordvec, len(word_to_index), dim_hidden, batch_size,\n",
    "                    input_sequence_length, output_sequence_length, learning_rate)\n",
    "    optimizer, place_holders, predictions, logits, losses = model.build_model()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    #restore checkpoint if continuing from a previous run or initialise the graph\n",
    "    if checkpoint:\n",
    "        saver.restore(sess, path)\n",
    "        print(\"checkpoint restored at path: {}\".format(path))\n",
    "    else:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "    #start iterating through episodes and batches to train\n",
    "    for epoch in range(epochs):\n",
    "        n_batch = dr.get_batch_num(batch_size=batch_size)\n",
    "        for batch in range(n_batch):\n",
    "\n",
    "            batch_input, batch_target = dr.generate_training_batch(batch_size)\n",
    "\n",
    "            #the batch input has the list of words from the training sete. The batch target has a list of sentences for the input which will be the target.\n",
    "            # the list of words is conerted to vector form using helper it then makes the feed dictionary for the graph\n",
    "            inputs_ = make_batch_input(batch_input, input_sequence_length, dim_wordvec, word_vector)\n",
    "\n",
    "            targets, masks = make_batch_target(batch_target, word_to_index, output_sequence_length)\n",
    "            feed_dict = {\n",
    "                place_holders['word_vectors']: inputs_,\n",
    "                place_holders['caption']: targets,\n",
    "                place_holders['caption_mask']: masks\n",
    "            }\n",
    "            #calls optimiser by feeding in training data and logs loss value to see progression\n",
    "            _, loss_val, preds = sess.run([optimizer, losses[\"entropy\"], predictions],\n",
    "                                          feed_dict=feed_dict)\n",
    "\n",
    "            if batch % display_interval == 0:\n",
    "                print(preds.shape)\n",
    "                print(\"Epoch: {}, batch: {}, loss: {}\".format(epoch, batch, loss_val))\n",
    "                print(\"===========================================================\")\n",
    "\n",
    "        saver.save(sess, path)\n",
    "        print(\"Model saved at {}\".format(path))\n",
    "    print(\"Training Complete.\")\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modes are restored and trainined again to create the chatbot\n",
    "def pg_train(epochs=1, checkpoint=False):\n",
    "    tf.reset_default_graph()\n",
    "    path = \"model/reinforcement/seq2seq\"\n",
    "    word_to_index, index_to_word, _ = preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
    "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "    generic_caption, generic_mask = generic_batch(generic_responses, batch_size, word_to_index,\n",
    "                                                    output_sequence_length)\n",
    "    dr = Data_Reader()\n",
    "    forward_graph = tf.Graph()\n",
    "    reverse_graph = tf.Graph()\n",
    "    default_graph = tf.get_default_graph()\n",
    "\n",
    "    # create two graphs to load the trained model\n",
    "    with forward_graph.as_default():\n",
    "        pg_model = Chatbot(dim_wordvec, len(word_to_index), dim_hidden, batch_size,\n",
    "                           input_sequence_length, output_sequence_length, learning_rate, policy_gradients=True)\n",
    "        optimizer, place_holders, predictions, logits, losses = pg_model.build_model()\n",
    "        sess = tf.InteractiveSession()\n",
    "        saver = tf.train.Saver()\n",
    "        if checkpoint:\n",
    "            saver.restore(sess, path)\n",
    "            print(\"checkpoint restored at path: {}\".format(path))\n",
    "        else:\n",
    "            tf.global_variables_initializer().run()\n",
    "            saver.restore(sess, 'model/forward/seq2seq')\n",
    "        # tf.global_variables_initializer().run()\n",
    "        with reverse_graph.as_default():\n",
    "            model = Chatbot(dim_wordvec, len(word_to_index), dim_hidden, batch_size,\n",
    "                            input_sequence_length, output_sequence_length, learning_rate)\n",
    "            _, rev_place_holders, _, _, reverse_loss = model.build_model()\n",
    "            sess2 = tf.InteractiveSession()\n",
    "            saver2 = tf.train.Saver()\n",
    "\n",
    "            saver2.restore(sess2, \"model/reverse/seq2seq\")\n",
    "            print(\"reverse model restored\")\n",
    "\n",
    "        dr = Data_Reader(load_list=True)\n",
    "\n",
    "    # load data to train in batches\n",
    "    for epoch in range(epochs):\n",
    "        n_batch = dr.get_batch_num(batch_size=batch_size)\n",
    "        for batch in range(n_batch):\n",
    "\n",
    "            batch_input, batch_caption, prev_utterance = dr.generate_training_batch_with_former(batch_size)\n",
    "            targets, masks = make_batch_target(batch_caption, word_to_index, output_sequence_length)\n",
    "            inputs_ = make_batch_input(batch_input, input_sequence_length, dim_wordvec, word_vector)\n",
    "\n",
    "            word_indices, probabilities = sess.run([predictions, logits],\n",
    "                                                   feed_dict={place_holders['word_vectors']: inputs_\n",
    "\n",
    "                                                       , place_holders[\"caption\"]: targets})\n",
    "\n",
    "            sentence = [index2sentence(generated_word, probability, index_to_word) for\n",
    "                        generated_word, probability in zip(word_indices, probabilities)]\n",
    "\n",
    "            word_list = [word.split() for word in sentence]\n",
    "\n",
    "            generic_test_input = make_batch_input(word_list, input_sequence_length, dim_wordvec, word_vector)\n",
    "\n",
    "            forward_coherence_target, forward_coherence_masks = make_batch_target(sentence,\n",
    "                                                                                    word_to_index,\n",
    "                                                                                    output_sequence_length)\n",
    "\n",
    "            generic_loss = 0.0\n",
    "\n",
    "            #learns when to say generic texts\n",
    "            for response in generic_test_input:\n",
    "                sentence_input = np.array([response] * batch_size)\n",
    "                feed_dict = {place_holders['word_vectors']: sentence_input,\n",
    "                            place_holders['caption']: generic_caption,\n",
    "                            place_holders['caption_mask']: generic_mask,\n",
    "                            }\n",
    "                generic_loss_i = sess.run(losses[\"entropy\"], feed_dict=feed_dict)\n",
    "                generic_loss -= generic_loss_i / batch_size\n",
    "\n",
    "            # print(\"generic loss work: {}\".format(generic_loss))\n",
    "\n",
    "            feed_dict = {place_holders['word_vectors']: inputs_,\n",
    "                        place_holders['caption']: forward_coherence_target,\n",
    "                        place_holders['caption_mask']: forward_coherence_masks,\n",
    "                        }\n",
    "\n",
    "            forward_entropy = sess.run(losses[\"entropy\"], feed_dict=feed_dict)\n",
    "\n",
    "            previous_utterance, previous_mask = make_batch_target(prev_utterance,\n",
    "                                                                    word_to_index, output_sequence_length)\n",
    "\n",
    "            feed_dict = {rev_place_holders['word_vectors']: generic_test_input,\n",
    "                        rev_place_holders['caption']: previous_utterance,\n",
    "                        rev_place_holders['caption_mask']: previous_mask,}\n",
    "            reverse_entropy = sess2.run(reverse_loss[\"entropy\"], feed_dict=feed_dict)\n",
    "\n",
    "            rewards = 1 / (1 + np.exp(-reverse_entropy - forward_entropy - generic_loss))\n",
    "\n",
    "            feed_dict = {place_holders['word_vectors']: inputs_,\n",
    "                        place_holders['caption']: targets,\n",
    "                        place_holders['caption_mask']: masks,\n",
    "                        place_holders['rewards']: rewards}\n",
    "            _, loss_pg, loss_ent = sess.run([optimizer, losses[\"pg\"], losses[\"entropy\"]], feed_dict=feed_dict)\n",
    "\n",
    "            if batch % display_interval == 0:\n",
    "                print(\"Epoch: {}, batch: {}, Entropy loss: {}, Policy gradient loss: {}\".format(epoch, batch, loss_ent, loss_pg))                                                              \n",
    "                print(\"rewards: {}\".format(rewards))\n",
    "                print(\"===========================================================\")\n",
    "        saver.save(sess, path)\n",
    "        print(\"Model saved at {}\".format(path))\n",
    "    print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy gradient trained to avoid.\n",
    "generic_responses = [\n",
    "    \"I don't know what you're talking about.\", \n",
    "    \"I don't know.\", \n",
    "    \"You don't know.\",\n",
    "    \"You know what I mean.\", \n",
    "    \"I know what you mean.\", \n",
    "    \"You know what I'm saying.\",\n",
    "    \"You don't know anything.\"\n",
    "]\n",
    "\n",
    "#training parameters\n",
    "checkpoint = True\n",
    "\n",
    "forward_model_path = 'model/forward'\n",
    "reversed_model_path = 'model/reversed'\n",
    "rl_model_path = \"model/rl\"\n",
    "model_name = 'seq2seq'\n",
    "word_count_threshold = 20\n",
    "reversed_word_count_threshold = 6\n",
    "dim_wordvec = 300\n",
    "dim_hidden = 1000\n",
    "input_sequence_length = 22\n",
    "output_sequence_length = 22\n",
    "learning_rate = 0.0001\n",
    "epochs = 1\n",
    "batch_size = 200\n",
    "\n",
    "display_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing word counts and creating vocab based on word count threshold 20\n",
      "filtered words from 76029 to 6847\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newlo\\AppData\\Local\\Temp\\ipykernel_14504\\4122317381.py:4: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell([tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.LSTMCell(lstm_size), keep_prob) for _ in range(num_layers)])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\repos\\ai\\rl-project-06-chatbot\\chatbot.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#train a feed forward model then a reverse model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#direction type, #of epochs\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(\u001b[39m'\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m50\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(\u001b[39m'\u001b[39m\u001b[39mreverse\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pg_train(\u001b[39m100\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32md:\\Projects\\repos\\ai\\rl-project-06-chatbot\\chatbot.ipynb Cell 37\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(type_, epochs, checkpoint)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#build chatbot model and pass in variables defined earlier.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m Chatbot(dim_wordvec, \u001b[39mlen\u001b[39m(word_to_index), dim_hidden, batch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                 input_sequence_length, output_sequence_length, learning_rate)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer, place_holders, predictions, logits, losses \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mbuild_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m saver \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mSaver()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m sess \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mInteractiveSession()\n",
      "\u001b[1;32md:\\Projects\\repos\\ai\\rl-project-06-chatbot\\chatbot.ipynb Cell 37\u001b[0m in \u001b[0;36mChatbot.build_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     word_vectors, caption, caption_mask \u001b[39m=\u001b[39m model_inputs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     place_holders \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mword_vectors\u001b[39m\u001b[39m'\u001b[39m: word_vectors,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m: caption,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcaption_mask\u001b[39m\u001b[39m'\u001b[39m: caption_mask}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m enc_output, enc_state \u001b[39m=\u001b[39m encoding_layer(word_vectors, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkeep_prob, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#dec_inp = bos_inclusion(caption, self.batch_size)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m dec_inp \u001b[39m=\u001b[39m caption\n",
      "\u001b[1;32md:\\Projects\\repos\\ai\\rl-project-06-chatbot\\chatbot.ipynb Cell 37\u001b[0m in \u001b[0;36mencoding_layer\u001b[1;34m(word_vectors, lstm_size, num_layers, keep_prob, vocab_size)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#outputs, state = tf.nn.rnn_cell.dynamic_rnn(cells, word_vectors, dtype=tf.float32)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m rnn_layer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mRNN([tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLSTMCell(\u001b[39m128\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                 tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLSTMCell(\u001b[39m256\u001b[39m)],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                 return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                 return_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m outputs, state \u001b[39m=\u001b[39m rnn_layer(cells, word_vectors, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/repos/ai/rl-project-06-chatbot/chatbot.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m outputs, state\n",
      "File \u001b[1;32md:\\Projects\\repos\\ai\\venv\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:573\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[39mif\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    572\u001b[0m   kwargs[\u001b[39m'\u001b[39m\u001b[39mconstants\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m constants\n\u001b[1;32m--> 573\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(RNN, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\repos\\ai\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Projects\\repos\\ai\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1347\u001b[0m, in \u001b[0;36mTensorShape.as_list\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1338\u001b[0m \u001b[39m\"\"\"Returns a list of integers or `None` for each dimension.\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \n\u001b[0;32m   1340\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39m  ValueError: If `self` is an unknown shape with an unknown rank.\u001b[39;00m\n\u001b[0;32m   1345\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1347\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mas_list() is not defined on an unknown TensorShape.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1348\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dims)\n",
      "\u001b[1;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "#train a feed forward model then a reverse model\n",
    "#direction type, #of epochs\n",
    "train('forward', 50, False)\n",
    "train('reverse', 50, False)\n",
    "pg_train(100, False)\n",
    "\n",
    "#https://researchdatapod.com/how-to-solve-python-attributeerror-module-tensorflow-has-no-attribute-placeholder/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model\n",
    "Once trained we test the chatbot on unseen dialogue. The bot is trying to be reasonably coherent. Context is important, depending on dataset used, responses will be in that context. Performance is measured as informativeness, high coherence and simplicity in answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and model\n",
    "def test(model_path=forward_model_path):\n",
    "    testing_data = open(path_to_questions, 'r').read().split('\\n')\n",
    "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "\n",
    "    _, index_to_word, _ = preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
    "\n",
    "    model = Chatbot(dim_wordvec, len(index_to_word), dim_hidden, batch_size,\n",
    "                            input_sequence_length, target_sequence_length, Training=False)\n",
    "\n",
    "    place_holders, predictions, logits = model.build_model()\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "#open response file and prepare list of questions\n",
    "    with open(responses_path, 'w') as out:\n",
    "\n",
    "        for idx, question in enumerate(testing_data):\n",
    "            print('question =>', question)\n",
    "\n",
    "            question = [refine(w) for w in question.lower().split()]\n",
    "            question = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in question]\n",
    "            question.insert(0, np.random.normal(size=(dim_wordvec,)))  # insert random normal at the first step\n",
    "\n",
    "            if len(question) > input_sequence_length:\n",
    "                question = question[:input_sequence_length]\n",
    "            else:\n",
    "                for _ in range(input_sequence_length - len(question)):\n",
    "                    question.append(np.zeros(dim_wordvec))\n",
    "\n",
    "            question = np.array([question])\n",
    "\n",
    "            feed_dict = {place_holders[\"word_vectors\"]: np.concatenate([question] * 2, 0),\n",
    "                         }\n",
    "\n",
    "            word_indices, prob_logit = sess.run([predictions, logits], feed_dict=feed_dict)\n",
    "\n",
    "            # print(word_indices[0].shape)\n",
    "            generated_sentence = index2sentence(word_indices[0], prob_logit[0], index_to_word)\n",
    "\n",
    "            print('generated_sentence =>', generated_sentence)\n",
    "            out.write(generated_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference path to trained models\n",
    "reinforcement_model_path = \"model/reinforcement/seq2seq\"\n",
    "forward_model_path = \"model/forward/seq2seq\"\n",
    "reverse_model_path = \"model/reverse/seq2seq\"\n",
    "\n",
    "# reference to sample questions and responses\n",
    "path_to_questions = 'results/sample_input.txt'\n",
    "responses_path = 'results/sample_output_RL.txt'\n",
    "\n",
    "#define model hyper parameters\n",
    "word_count_threshold = 20\n",
    "dim_wordvec = 300\n",
    "dim_hidden = 1000\n",
    "input_sequence_length = 25\n",
    "target_sequence_length = 22\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(reinforcement_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fcafb2413c669541f99831941a53394b43f3aa639141e36df9bf58e7aa89d43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
